## Unbiased Estimators

Let $\hat{\theta}$ be a point estimator of some unknown population parameter $\theta$. It would be ideal if the sampling distribution of $\hat{\theta}$ would have a mean equal to $\theta$. In other words, if we were to take many random samples, the average of the $\hat{\theta}$s should be equal to our unknown parameter we are trying to estimate, $\theta$. A point estimate with this property is called an **unbiased estimator**.

**Definition:** A statistic $\hat{\theta}$ is an unbiased estimator of the parameter $\theta$ if
$$E(\hat{\theta})=0$$

Recall that 
1. For any random variable $x$ with mean $\mu = E(x)$, the variance is $\sigma^2=Var(x)=E[(x-\mu)^2]$
2. From the Central Limit Theorem, the sample mean, $\overline x$, is a random variable with mean = $\mu_{\overline x} = E(\overline x)=\mu$. This proved again in Example 1 (below) and variance = $\sigma^2_{\overline x}=Var(\overline x)=E[(\overline x - \mu)^2]=\frac{\sigma^2}{n}$
3. If $x_{1},x_{2},\dots,x_{n}$ are independent and identical (iid) random variables, then they have a common mean, $\mu=E(x_{i})$ and variance = $\sigma^2=Var(x_{i})=E[(x_{i}-\mu)^2]$
4. If $x_{1},x_{2},\dots,x_{n}$ are independent and identical (iid) random variables and if $a_{1},a_{2},\dots,a_{n}$ are any constants, then $E(a_{1}x_{1}+a_{2}x_{2}+\dots+a_{n}x_{n})=a_{1}E(x_{1})+x_{2}E(x_{2})+\dots+a_{n}E(x_{n})$ and $Var(a_{1}x_{1}+a_{2}x_{2}+\dots+a_{n}x_{n})=a_{1}^2Var(x_{1})+a_{2}^2Var(x_{2})+\dots+a_{n}^2Var(x_{n})$

## Examples

#### Example 1

**Problem:** Show that $\overline x$ is an unbiased estimator of the parameter $\mu$.

**Solution:** Let $x_{1},x_{2},\dots,x_{n}$ be a random sample of size $n$ selected independently from a population with mean $\mu$ and variance $\sigma^2$.
$$E(\overline x)=E\left( \frac{1}{n} \sum_{i=1}^n x_{i} \right)$$
$$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~= \frac{1}{n} \sum_{i=1}^n E(x_{i})~ ~ ~\mathrm{Recall~(4)~above}$$
$$=\frac{1}{n}\sum_{i=1}^n \mu$$

$$=\frac{1}{n}(n\mu)$$
$$=\mu$$

#### Example 2

**Problem:** Show that $s^2$ is an unbiased estimator of the parameter $\sigma^2$

**Solution:** Let $x_{1},x_{2},\dots,x_{n}$ be a random sample of size $n$ selected independently from a population with mean $\mu$ and variance $\sigma^2$.

Recall that sample variance is
$$s^2=\frac{\sum_{i=1}^n (x_{i}- \overline x)^2}{n-1}$$

We can show that
$$\sum_{i=1}^n(x_{i}- \overline x)^2=\sum_{i=1}^n(x_{i}-\mu)^2-n(\overline x-\mu)^2$$

By
$$\sum_{i=1}^n(x_{i}-\overline x)^2=\sum_{i=1}^n[(x_{i}-\mu)-(\overline x - \mu)]^2$$
$$=\sum_{i=1}^n(x_{i}-\mu)^2-2(\overline x - \mu)\sum_{i=1}^n(x_{i}-\mu)+\sum_{i=1}^n(x_{i}-\mu)^2$$
$$=\sum_{i=1}^n(x_{i}-\mu)^2-2(\overline x - \mu)[\sum_{i=1}^n(x_{i})-n\mu]+n(\overline x - \mu)^2$$
$$=\sum_{i=1}^n(x_{i}-\mu)^2-2n(\overline x - \mu)(\overline x - \mu)+n(\overline x - \mu)^2$$
$$=\sum_{i=1}^n(x_{i}-\mu)^2-n(\overline x - \mu)^2$$
Now,
$$E(s^2)=E[\frac{1}{n-1}\sum_{i=1}^n(x_{i}-\overline x)^2]$$
$$=\frac{1}{n-1}[=\sum_{i=1}^nE(x_{i}-\mu)^2-nE(\overline x - \mu)^2]$$
$$= \frac{1}{n-1}(=\sum_{i=1}^n\sigma_{x_{i}}^2-n\sigma_{\overline x}^2)$$
As recalled above in (3) and (2),
$$\sigma_{x_{i}}^2=\sigma^2 \mathrm{~ ~ ~ for~i~=~1,2,\dots,n}$$

<p align="center">and</p>

$$\sigma_{\overline x}^2=\frac{\sigma^2}{n}$$
Therefore,
$$E(s^2)=\frac{1}{n-1}(n\sigma^2-n\frac{\sigma^2}{n})$$
$$=\frac{1}{n-1}\sigma^2(n-1)$$
$$=\sigma^2$$

> [!note]
> 
> Example 2 explains why we divide by $n−1$ rather than n when calculating the sample variance. Dividing by $n−1$ makes the sample variance an unbiased estimator of the population variance.
> 
> Notably, while $s^2$ is an unbiased estimator of $\sigma^2$, $s$ (sample standard deviation) is NOT an unbiased estimator of $\sigma$. However, the bias becomes insignificant for large samples.

