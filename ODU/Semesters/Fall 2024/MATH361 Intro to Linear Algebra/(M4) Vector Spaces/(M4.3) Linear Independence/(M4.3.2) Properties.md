## Properties

| The $n$-vectors $\vec{u_{1}},\dots,\vec{u_{k}}$ are | The equation $c_{1} \vec{u_{1}}+\dots+c_{k} \vec{u_{k}}=\vec{0}$ can be solved |
| --------------------------------------------------- | ------------------------------------------------------------------------------ |
| linearly independent<br>                            | only by $c_{1}=\dots+c_{k}=0$                                                  |
| linearly dependent                                  | with at least some nonzero $c_{i}$'s                                           |

| The $n$-vectors $\vec{u_{1}},\dots,\vec{u_{k}}$ are | The linear system $\begin{bmatrix}\|&\dots&\| \\ \vec{u_{1}} &\dots&u_{k} \\ \| & \dots & \|\end{bmatrix}\begin{bmatrix}c_{1} \\ \vdots \\ c_{k}\end{bmatrix}=\begin{bmatrix}0 \\ \vdots \\ 0\end{bmatrix}$ has |
| --------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| linearly independent<br>                            | only the trivial solution                                                                                                                                                                                       |
| linearly dependent                                  | the trivial solution as well as nontrivial solutions                                                                                                                                                            |

We can add another property to our list of equivalent statements:

### 6 Equivalent Statements

For an $n \times n$ matrix $A$, the following statements are equivalent:

1. $A$ is nonsingular
2. $A$ is row equivalent to $I_{n}$
3. For every $n$-vector $\vec{b}$, the system $A \vec{x} = \vec{b}$ has a unique solution
4. The system $A \vec{x} = \vec{0}$ has only the trivial solution
5. $\det A \neq 0$
6. The columns of $A$ are linearly independent

### 5 Equivalent Negative Statements

Likewise, we can do the same for the negative statements:

1. $A$ is singular
2. $A$ is not row equivalent to $I_{n}$
3. For some $n$-vector $\vec{b}$, the system $A \vec{x} = \vec{b}$ has either no solution or many solutions
4. The system $A \vec{x} = \vec{0}$ has only nontrivial solutions
5. $\det A = 0$
6. The columns of $A$ are linearly dependent

### Example

Are the vectors $\begin{bmatrix}1&0\\0&1\end{bmatrix},\begin{bmatrix}0&1\\1&0\end{bmatrix},\begin{bmatrix}1&0\\0&-1\end{bmatrix}$ L.I.?

i.e., does the system $c_{1} \begin{bmatrix}1&0 \\ 0&1\end{bmatrix}+c_{2} \begin{bmatrix}0&1 \\ 1&0\end{bmatrix}+c_{3} \begin{bmatrix}1&0 \\ 0&-1\end{bmatrix}=\begin{bmatrix}0&0 \\ 0&0\end{bmatrix}$ have a unique solution?

**Solution:**

$c_{1} \begin{bmatrix}1&0 \\ 0&1\end{bmatrix}+c_{2} \begin{bmatrix}0&1 \\ 1&0\end{bmatrix}+c_{3} \begin{bmatrix}1&0 \\ 0&-1\end{bmatrix}=\begin{bmatrix}0&0 \\ 0&0\end{bmatrix}$ can be rewritten as

$\begin{bmatrix}c_{1}+c_{3} & c_{2} \\ c_{2} & c_{1}-c_{3}\end{bmatrix}=\begin{bmatrix}0&0 \\ 0&0\end{bmatrix}$

for two matrices to be equal, their corresponding entries must be equal:

$c_{1}+c_{3}=0$
$c_{2}=0$
$c_{2}=0$
$c_{1}-c_{3}=0$

$$
\begin{bmatrix}
1&0&1&|&0 \\
0&1&0&|&0 \\
0&1&0&|&0 \\
1&0&-1&|&0
\end{bmatrix}
$$

is row equivalent to

$$
\begin{bmatrix}
\boxed{1}&0&0&|&0 \\
0&\boxed{1}&0&|&0 \\
0&0&\boxed{1}&|&0 \\
0&0&0&|&0
\end{bmatrix}
$$

in r.r.e.f.

Because of the 3 leading columns, there is nothing arbitrary, meaning there is a unique solution

$c_{1} \begin{bmatrix}1&0 \\ 0&1\end{bmatrix}+c_{2} \begin{bmatrix}0&1 \\ 1&0\end{bmatrix}+c_{3} \begin{bmatrix}1&0 \\ 0&-1\end{bmatrix}=\begin{bmatrix}0&0 \\ 0&0\end{bmatrix}$ requires $c_{1}=c_{2}=c_{3}=0$

Therefore, the set of vectors is _L.I._

### Example 2

Are the vectors in $P_{2}$, $p_{1}(t)=1+t^2,~p_{2}(t)=1+t,~p_{3}(t)=t+t^2,~p_{4}(t)=t-t^2$ linearly independent?

i.e., does the system $c_{1} (1+t^2) + c_{2}(1+t) + c_{3}(t+t^2)+c_{4}(t-t^2)=0$ have a unique solution?

**Solution:**

$c_{1} (1+t^2) + c_{2}(1+t) + c_{3}(t+t^2)+c_{4}(t-t^2)=0$

$\to c_{1}+c_{2}+(c_{2}+c_{3}+c_{4})t+(c_{1}+c_{3}-c_{4})t^2=0$

The two polynomials being equal for all $t$ requires that coefficients corresponding to like powers on both sides must be equal:

$c_{1}+c_{2}=0$
$c_{2}+c_{3}+c_{4}=0$
$c_{1}+c_{3}-c_{4}=0$

$$
\begin{bmatrix}
1&1&0&0&|&0 \\
0&1&1&1&|&0 \\
1&0&1&-1&|&0
\end{bmatrix}
$$

Gives us

$$
\begin{bmatrix}
\boxed{1}&0&0&-1&|&0 \\
0&\boxed{1}&0&1&|&0 \\
0&0&\boxed{1}&0&|&0 \\
\end{bmatrix}
$$

in r.r.e.f.

No leading entry in the fourth column, meaning $c_{4}$ can be arbitrary. There are many solutions.

Therefore, $p_{1}(t),p_{2}(t),p_{3}(t),p_{4}(t)$ are _L.D._

### Theorem 4.8

Let $\vec{u_{1}}$ and $\vec{u_{2}}$ be vectors in a vector space $V$.

1. $S=\{\vec{u_{1}}\}$ is linearly dependent if and only if $\vec{u_{1}}=\vec{0}$ (zero vector)
2. $S=\{\vec{u_{1}}, \vec{u_{2}}\}$ is linearly dependent if and only if one of the vectors is a scalar multiple of the other

### Theorem 4.9

Let $S$ be an indexed set of $m$ vectors in a vector space $V$. If there exists a linearly dependent subset of $S$ then $S$ itself must be linearly dependent.

### Corollary 4.10

Let $S=\{\vec{u_{1}},\dots,\vec{u_{k}}\}$ be an indexed set of vectors in a vector space $V$.

1. If at least one of the vectors $\vec{u_{1}},\dots,\vec{u_{k}}$ is a zero vector then $S$ is linearly dependent
2. If a vector appears more than once in $S$ then $S$ is linearly dependent

### Theorem 4.11

For any integer $k>1$, the vectors $\vec{u_{1}},\dots,\vec{u_{k}}$ in a vector space $V$ are linearly dependent if and only if at least one of them can be expressed as a linear combination of the remaining vectors.

### Example 2 Revisited

Recall:

$p_{1}(t)=1+t^2,$
$p_{2}(t)=1+t,$
$p_{3}(t)=t+t^2,$
$p_{4}(t)=t-t^2$

are linearly dependent

$$
\begin{bmatrix}
\boxed{1}&0&0&-1&|&0 \\
0&\boxed{1}&0&1&|&0 \\
0&0&\boxed{1}&0&|&0 \\
\end{bmatrix}
$$

Notice the leading columns vs. the non-leading column that makes $c_{4}$ arbitrary. We can say

$c_{1}=c_{4}$
$c_{2}=-c_{4}$
$c_{3}=0$
$c_{4}$ is arbitrary

$c_{1} \underbrace{(1+t^2)}_{p_{1}(t)}+c_{2} \underbrace{1+t}_{p_{2}(t)}+c_{3} \underbrace{t+t^2}_{p_{3}(t)}+c_{4} \underbrace{(t-t^2)}_{p_{4}(t)}=0$

$c_{1}=1$
$c_{2}=-1$
$c_{3}=0$
e.g., let $c_{4}=1$

^ This allows us to take a polynomial, particularly $p_{4}$, and express it as a linear combination of the remaining vectors.

### Theorem 4.12

The vectors $\vec{u_{1}},\dots,\vec{u_{k}}$ in a vector space $V$ are _L.D._ if and only if
- $\vec{u_{1}}=\vec{0}$
or
- one of the vectors $u_{2},\dots,u_{k}$ can be expressed as a linear combination of the preceding vectors.