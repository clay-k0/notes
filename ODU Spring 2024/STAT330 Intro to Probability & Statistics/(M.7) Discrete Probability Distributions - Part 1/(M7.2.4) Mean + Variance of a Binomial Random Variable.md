- - -
The following shows how a binomial random variable can be represented by a sum of $n$ independent and identical (iid) random variables. From this representation, we can develop formulas for the mean, variance, and standard deviation of a binomial distribution.

Consider $n$ iid trials, and let $X_{i}$ equal the number of successes occurring on the $i^{th}$ trial

$$x_{i}=\begin{cases}1,&\, if~a~success~occurs\\ 0,&\ if~a~failure~occurs\end{cases}$$
$X_{i}$ is called a Bernoulli trial. The probability distribution of $X_{i}$ is:

| $x_{i}$ | 0 | 1 |
| ---- | ---- | ---- |
| $f(x_{i})$ | $1-p$ | $p$ |

The expected value of $X_{i}$ is
$E(X_{i})=\sum_{x_{i}}x_{i} \times f(x_{i})=0(1-p)+1(p)=p$

The variance of $X_{i}$ is
$Var(X_{i})=E(X_{i}^2)-(E(X_{i}))^2=\sum_{x_{i}}x_{i}^2 \times f(x_{i})-p^2$
$=0^2(1-p)+1^2(p)-p^2$
$=p-p^2$
$=p(1-p)$
$=pq$

Let $Y=X_{1}+X_{2}+\dots+X_{n}$.  Each $X_{i}$ determines if the $i^{th}$ item in the sample is a success or not therefore, $Y$ is equal to the sum of the $X_{i}'s$ counts the number of successes in the $n$ trials. Hence, $Y$ is a binomial random variable. 

Using Theorem 9 from Module 5 (Summation of $n$ Independent Random Variables), the expected value of $Y$ is
$E(Y)=E(X_{1}+X_{2}+\dots+X_{n})$
$=E(X_{1})+E(X_{2})+\dots+E(X_{n})$
$=p+p+\dots+p$
$=np$

Again, using Theorem 9 from Module 5, the variance of $Y$ is
$Var(Y)=Var(X_{1}+X_{2}+\dots+X_{n})$
$=Var(X_{1})+Var(X_{2})+\dots+Var(X_{n})$
$=pq+pq+\dots+pq$
$=npq$

Because $Y$ is a binomial random variable, we have the following properties of a binomial distribution:
$$Mean:~\mu=np$$
$$Variance:~\sigma^2=npq$$
$$Standard~Deviation:~\sigma=\sqrt{ \sigma^2 }=\sqrt{ npq }$$

